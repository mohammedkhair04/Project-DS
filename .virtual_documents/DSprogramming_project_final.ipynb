


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns





df = pd.read_csv('all_stocks_5yr.csv')


df['date'] = pd.to_datetime(df['date'])


df.head() # to first 5 or select number
# df.tail() to last 5 or select number








df.info()





df.shape


print(f'Number of rows: {df.shape[0]}')
print(f'Number of columns: {df.shape[1]}')





plt.title('NULL VALUE')
sns.heatmap(df.isnull(),cmap='mako')
plt.show()





df.isnull().sum()
# This null value cant be seen in heatmap bcz it small number





df_cleaned = df.dropna(subset=['open', 'high', 'low'])





df_cleaned.isnull().sum()


df_cleaned.head()





df_cleaned.shape


df_cleaned.info()





print(f'any duplicated value: {df_cleaned.duplicated().any()}')





df_cleaned.to_json('df_cleaned.json', orient='records')





df_cleaned.describe()


# df_cleaned.dropna()


from sklearn.preprocessing import LabelEncoder,MinMaxScaler


scaler = MinMaxScaler()
numeric_columns = ['open', 'high', 'low', 'close', 'volume']
df_cleaned[numeric_columns] = scaler.fit_transform(df_cleaned[numeric_columns])


# le = LabelEncoder()
# labels = le.fit_transform(df_cleaned['Name'])

# # Convert to binary manually
# binary_encoded = [list(map(int, bin(x)[2:].zfill(10))) for x in labels]
# binary_df = pd.DataFrame(binary_encoded, columns=[f'bit_{i}' for i in range(10)])
# binary_df


df_cleaned['daily_return'] = df.groupby('Name')['close'].pct_change() * 100


# pip install category_encoders


# pip install --upgrade category_encoders


# import category_encoders as ce


df_cleaned.head()


df_cleaned.columns


df_filtered = df_cleaned[(df_cleaned['date'] >= '2014-01-01') & (df_cleaned['date'] <= '2015-12-31')]
df_filtered


plt.figure(figsize=(15, 8))
top_stocks = df_cleaned['Name'].value_counts().head(5).index
for stock in top_stocks:
    stock_data = df_cleaned[df_cleaned['Name'] == stock]
    plt.plot(stock_data['date'], stock_data['close'], label=stock)
plt.title('Scaled Price Trends for Top 5 Stocks')
plt.xlabel('Date')
plt.ylabel('Scaled Price')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


plt.figure(figsize=(12, 6))
sns.countplot(data=df_cleaned, x='Name')
plt.title('Distribution of Binary Encoded Stock Names')
plt.xlabel('Binary Encoded Stock Name')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


stock_data = df_cleaned[df_cleaned['Name'] == 'AAPL'].copy()


plt.figure(figsize=(15, 10))
plt.subplot(2, 1, 1)
plt.plot(stock_data['date'], stock_data['volume'], color='green')
plt.title('Scaled Trading Volume Over Time')
plt.xlabel('Date')
plt.ylabel('Scaled Volume')
plt.grid(True)
plt.xticks(rotation=45)

plt.subplot(2, 1, 2)
sns.histplot(data=stock_data, x='volume', bins=50, color='green')
plt.title('Scaled Volume Distribution')
plt.xlabel('Scaled Volume')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()


stock_data['MA20'] = stock_data['close'].rolling(window=20).mean()
stock_data['MA50'] = stock_data['close'].rolling(window=50).mean()
plt.figure(figsize=(15, 8))
plt.plot(stock_data['date'], stock_data['close'], label='Scaled Close Price', alpha=0.7)
plt.plot(stock_data['date'], stock_data['MA20'], label='20-day MA', color='red')
plt.plot(stock_data['date'], stock_data['MA50'], label='50-day MA', color='green')
plt.title('Scaled Price with Moving Averages')
plt.xlabel('Date')
plt.ylabel('Scaled Price')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


top_stocks_data = df_cleaned[df_cleaned['Name'].isin(top_stocks)].pivot(
    index='date', 
    columns='Name', 
    values='close'
)
correlation_matrix = top_stocks_data.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, 
            annot=True, 
            cmap='coolwarm', 
            center=0,
            fmt='.2f')
plt.title('Scaled Stock Price Correlation Matrix')
plt.tight_layout()
plt.show()


plt.figure(figsize=(12, 8))
plt.scatter(df_cleaned['volume'], 
           df_cleaned['close'], 
           alpha=0.5, 
           c=df_cleaned['daily_return'], 
           cmap='viridis')
plt.colorbar(label='Daily Return (%)')
plt.title('Scaled Volume vs Price (colored by Daily Return)')
plt.xlabel('Scaled Volume')
plt.ylabel('Scaled Price')
plt.grid(True)
plt.tight_layout()
plt.show()


import threading
from queue import Queue
import pandas as pd
import numpy as np
from threading import Lock
import os
import psutil

# Function to determine optimal number of threads
def get_optimal_thread_count():
    # Get system memory info
    memory = psutil.virtual_memory()
    print(f'system memory info:{memory}')
    # Get available CPU cores
    cpu_cores = os.cpu_count()
    print(f'available CPU cores:{cpu_cores}')
    
    # Calculate based on available memory and CPU cores
    memory_based = max(1, memory.available // (1 * 1024 * 1024 * 1024))  # 2GB per thread
    
    # Use the minimum of memory-based and CPU-based thread count
    optimal_threads = min(memory_based, cpu_cores)
    
    print(f"System has {cpu_cores} CPU cores and {memory.available / (1024**3):.1f}GB available memory")
    print(f"Using {optimal_threads} threads for processing")
    
    return optimal_threads

# Create a thread-safe queue to store results
result_queue = Queue()
print_lock = Lock()

# Function to process a chunk of data
def process_stock_chunk(chunk, chunk_id):
    try:
        # Calculate metrics
        metrics = {
            'chunk_id': chunk_id,
            'mean_close': chunk['close'].mean(),
            'mean_volume': chunk['volume'].mean(),
            'volatility': chunk['high'].max() - chunk['low'].min(),
            'daily_returns': chunk['close'].pct_change().mean() * 100,
            'trading_range': (chunk['high'] - chunk['low']).mean()
        }
        
        # Add results to queue
        result_queue.put(metrics)
        
        # Print progress with lock to prevent messy output
        with print_lock:
            print(f'Processed chunk {chunk_id}')
            
    except Exception as e:
        with print_lock:
            print(f'Error processing chunk {chunk_id}: {e}')

# Get optimal number of threads
num_threads = get_optimal_thread_count()

# Calculate chunk size based on data size and number of threads
total_rows = len(df)
chunk_size = max(1000, total_rows // num_threads)  # Ensure minimum chunk size of 1000
num_chunks = (total_rows + chunk_size - 1) // chunk_size  # Round up division

print(f"Processing {total_rows} rows in {num_chunks} chunks of approximately {chunk_size} rows each")

threads = []

# Create and start threads
for i in range(num_chunks):
    start_idx = i * chunk_size
    end_idx = min(start_idx + chunk_size, total_rows)  # Ensure we don't exceed dataframe length
    chunk = df.iloc[start_idx:end_idx]
    
    thread = threading.Thread(
        target=process_stock_chunk,
        args=(chunk, i)
    )
    threads.append(thread)
    thread.start()

# Wait for all threads to complete
for thread in threads:
    thread.join()

# Collect results from queue
results = []
while not result_queue.empty():
    results.append(result_queue.get())

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Sort by volatility
results_df = results_df.sort_values('volatility', ascending=False)

# Display results
print("\nAnalysis Results:")
print(results_df)

# Calculate overall statistics
print("\nOverall Statistics:")
print(f"Average Daily Returns: {results_df['daily_returns'].mean():.2f}%")
print(f"Average Trading Range: ${results_df['trading_range'].mean():.2f}")
print(f"Average Volume: {results_df['mean_volume'].mean():.0f}")


df_cleaned['daily_return'] = df_cleaned['close'].pct_change()


high_volatility_days = df_cleaned[abs(df_cleaned['daily_return']) > 0.05] # أيام التقلبات العالية
high_volatility_days


df_cleaned.head()
# First value is null because no day before first day(not problem)


def sort_day_by_volume(df):
    df = df.sort_values(by='volume', ascending=False)
    return df
print(sort_day_by_volume(df_cleaned))





import requests
from bs4 import BeautifulSoup
import json

# Target URL
url = "https://finance.yahoo.com/most-active"

# Headers to mimic browser
headers = {
    'User-Agent': 'Mozilla/5.0'
}

# Fetch the page
response = requests.get(url, headers=headers)

# Parse the page
soup = BeautifulSoup(response.text, "html.parser")

# Locate the stock table
table = soup.find("table")
rows = table.find_all("tr")[1:]  # Skip header row

data = []

# Extract each row
for row in rows:
    cols = row.find_all("td")
    if len(cols) < 6:
        continue  # skip incomplete rows

    stock = {
        "Symbol": cols[0].text.strip(),
        "Name": cols[1].text.strip(),
        "Price": cols[2].text.strip(),
        "Change": cols[3].text.strip(),
        "% Change": cols[4].text.strip(),
        "Volume": cols[5].text.strip()
    }
    data.append(stock)

with open("most_active_stocks.json", "w") as f:
    json.dump(data, f, indent=2)

print("Scraped data saved to most_active_stocks.json")
Scrape = pd.read_json('most_active_stocks.json')
Scrape.head()
